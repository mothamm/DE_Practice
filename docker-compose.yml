services:
  # ----------------------
  # Database + UI
  # ----------------------

  postgres:
    image: postgres:latest
    container_name: postgres_db
    restart: always
    environment:
      POSTGRES_DB: demo
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
    volumes:
      - psql_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
  
  mysql:
    image: mysql:latest
    container_name: mysql_db
    ports: ["3306:3306"]
    restart: always
    environment:
     MYSQL_ROOT_PASSWORD: admin
     MYSQL_DATABASE: demo
    volumes:
      - mysql_data:/var/lib/mysql

  dbeaver:
    image: dbeaver/cloudbeaver
    ports: ["8080:8978"]
    container_name: dbeaver-admin
    restart: unless-stopped
    volumes:
      - dbeaver_data:/opt/cloudbeaver/workspace
    tty: true
    depends_on:
      - postgres
      - mysql
    links:
      - postgres
      - mysql

  # pgadmin:
  #   image: dpage/pgadmin4:latest
  #   container_name: pgadmin
  #   restart: always
  #   environment:
  #     PGADMIN_DEFAULT_EMAIL: uub@ubuntu.com
  #     PGADMIN_DEFAULT_PASSWORD: boo
  #   ports:
  #     - "8080:80" # Host port 5050 maps to container port 80
  #   depends_on:
  #     - postgres
  #   volumes:
  #     - pgadmin_data:/var/lib/pgadmin

  # ----------------------
  # Jupyter for Python/ETL
  # ----------------------
  jupyter:
    image: quay.io/jupyter/datascience-notebook
    container_name: jupyter_lab
    restart: always
    environment:
      JUPYTER_TOKEN: lab123
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/

  # ----------------------
  # Airflow for Orchestration
  # ----------------------
  #  airflow:
  #    image: apache/airflow:2.9.1
  #    container_name: airflow
  #    restart: always
  #    environment:
  #      AIRFLOW__CORE__EXECUTOR: LocalExecutor
  #      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin@postgres:5432/demo
  #    ports:
  #      - "8081:8080"
  #    depends_on:
  #      - postgres
  #    volumes:
  #      - ./airflow_dags:/opt/airflow/dags   # store DAGs on your host
  #
  #  # ----------------------
  #  # Kafka (using Redpanda)
  #  # ----------------------
  #  redpanda:
  #    image: docker.redpanda.com/redpandadata/redpanda:v24.1.5
  #    container_name: redpanda
  #    command:
  #      - redpanda start
  #      - --overprovisioned
  #      - --smp 1
  #      - --memory 1G
  #      - --reserve-memory 0M
  #      - --node-id 0
  #      - --check=false
  #      - --kafka-addr PLAINTEXT://0.0.0.0:9092
  #      - --advertise-kafka-addr PLAINTEXT://redpanda:9092
  #    ports:
  #      - "9092:9092"   # Kafka broker
  #      - "9644:9644"   # Admin API
  #    volumes:
  #      - ./redpanda_data:/var/lib/redpanda   # Kafka logs/messages persisted
  #
  #  redpanda-console:
  #    image: docker.redpanda.com/redpandadata/console:v2.6.0
  #    container_name: redpanda_console
  #    restart: always
  #    environment:
  #      KAFKA_BROKERS: redpanda:9092
  #    ports:
  #      - "8082:8080"
  #    depends_on:
  #      - redpanda
  #
  #  # ----------------------
  #  # MinIO (S3-like Storage)
  #  # ----------------------
  #  minio:
  #    image: minio/minio:latest
  #    container_name: minio
  #    command: server /data --console-address ":9001"
  #    environment:
  #      MINIO_ROOT_USER: minio
  #      MINIO_ROOT_PASSWORD: minio123
  #    ports:
  #      - "9000:9000"   # S3 API
  #      - "9001:9001"   # Console UI
  #    volumes:
  #      - ./minio_storage:/data   # host-mounted folder for your S3 objects
  #
# ----------------------
# Volumes (if you want Docker-managed instead of host folders)
# ----------------------
volumes:
  psql_data:
  mysql_data:
  # pgadmin_data:
  dbeaver_data:
#   minio_data:
#   redpanda_data: